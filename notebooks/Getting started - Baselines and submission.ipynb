{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assigned-wiring",
   "metadata": {},
   "source": [
    "This notebook presents some code to compute some basic baselines.\n",
    "\n",
    "In particular, it shows how to:\n",
    "1. Use the provided validation set\n",
    "2. Compute the top-30 metric\n",
    "3. Save the predictions on the test in the right format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-baghdad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:34.825855Z",
     "start_time": "2022-02-15T15:34:33.902797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Change this path to adapt to where you downloaded the data\n",
    "DATA_PATH = Path(\"data\")\n",
    "\n",
    "# Create the path to save submission files\n",
    "SUBMISSION_PATH = Path(\"submissions\")\n",
    "os.makedirs(SUBMISSION_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-pastor",
   "metadata": {},
   "source": [
    "We also load the official metric, top-30 error rate, for which we provide efficient implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-italic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:34.831795Z",
     "start_time": "2022-02-15T15:34:34.827913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function top_30_error_rate in module GLC.metrics:\n",
      "\n",
      "top_30_error_rate(y_true, y_score)\n",
      "    Computes the top-30 error rate.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true: 1d array, [n_samples]\n",
      "        True labels.\n",
      "    y_score: 2d array, [n_samples, n_classes]\n",
      "        Scores for each label.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    float:\n",
      "        Top-30 error rate value.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Complexity: :math:`O( n_\\text{samples} \\times n_\\text{classes} )`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GLC.metrics import top_30_error_rate\n",
    "help(top_30_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "centered-pocket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:34.840459Z",
     "start_time": "2022-02-15T15:34:34.833489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function top_k_error_rate_from_sets in module GLC.metrics:\n",
      "\n",
      "top_k_error_rate_from_sets(y_true, s_pred)\n",
      "    Computes the top-k error rate from predicted sets.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true: 1d array, [n_samples]\n",
      "        True labels.\n",
      "    s_pred: 2d array, [n_samples, k]\n",
      "        Previously computed top-k sets for each sample.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    float:\n",
      "        Error rate value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GLC.metrics import top_k_error_rate_from_sets\n",
    "help(top_k_error_rate_from_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-field",
   "metadata": {},
   "source": [
    "For submissions, we will also need to predict the top-30 sets for which we also provide an efficient implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "little-boards",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:34.850192Z",
     "start_time": "2022-02-15T15:34:34.843048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function predict_top_30_set in module GLC.metrics:\n",
      "\n",
      "predict_top_30_set(y_score)\n",
      "    Predicts the top-30 sets from scores.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_score: 2d array, [n_samples, n_classes]\n",
      "        Scores for each sample and label.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    2d array, [n_samples, 30]:\n",
      "        Predicted top-30 sets for each sample.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Complexity: :math:`O( n_\\text{samples} \\times n_\\text{classes} )`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from GLC.metrics import predict_top_30_set\n",
    "help(predict_top_30_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-experiment",
   "metadata": {},
   "source": [
    "# Observation data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-brave",
   "metadata": {},
   "source": [
    "We first need to load the observation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bottom-flush",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:35.686811Z",
     "start_time": "2022-02-15T15:34:34.851926Z"
    }
   },
   "outputs": [],
   "source": [
    "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs = pd.concat((df_obs_fr, df_obs_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-organizer",
   "metadata": {},
   "source": [
    "Then, we retrieve the train/val split provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comic-privacy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:36.132790Z",
     "start_time": "2022-02-15T15:34:35.688156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 40080 (2.5% of train observations)\n"
     ]
    }
   ],
   "source": [
    "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
    "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
    "\n",
    "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
    "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
    "\n",
    "n_val = len(obs_id_val)\n",
    "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-lending",
   "metadata": {},
   "source": [
    "We also load the observation data for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "martial-adaptation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:36.169180Z",
     "start_time": "2022-02-15T15:34:36.134721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for testing: 36421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observation_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10782781</th>\n",
       "      <td>43.601788</td>\n",
       "      <td>6.940195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10364138</th>\n",
       "      <td>46.241711</td>\n",
       "      <td>0.683586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10692017</th>\n",
       "      <td>45.181095</td>\n",
       "      <td>1.533459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10222322</th>\n",
       "      <td>46.938450</td>\n",
       "      <td>5.298678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10241950</th>\n",
       "      <td>45.017433</td>\n",
       "      <td>0.960736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 latitude  longitude\n",
       "observation_id                      \n",
       "10782781        43.601788   6.940195\n",
       "10364138        46.241711   0.683586\n",
       "10692017        45.181095   1.533459\n",
       "10222322        46.938450   5.298678\n",
       "10241950        45.017433   0.960736"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
    "\n",
    "obs_id_test = df_obs_test.index.values\n",
    "\n",
    "print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
    "\n",
    "df_obs_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-lincoln",
   "metadata": {},
   "source": [
    "# Sample submission file\n",
    "\n",
    "In this section, we will demonstrate how to generate the sample submission file provided.\n",
    "\n",
    "To do so, we will use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "registered-playing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:36.174908Z",
     "start_time": "2022-02-15T15:34:36.171070Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(filename, observation_ids, s_pred):\n",
    "    s_pred = [\n",
    "        \" \".join(map(str, pred_set))\n",
    "        for pred_set in s_pred\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Id\": observation_ids,\n",
    "        \"Predicted\": s_pred\n",
    "    })\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-familiar",
   "metadata": {},
   "source": [
    "The sample submission consists in always predicting the first 30 species for all the test observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "minute-edgar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:36.185070Z",
     "start_time": "2022-02-15T15:34:36.176395Z"
    }
   },
   "outputs": [],
   "source": [
    "first_30_species = np.arange(30)\n",
    "s_pred = np.tile(first_30_species[None], (len(df_obs_test), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-pakistan",
   "metadata": {},
   "source": [
    "We can then generate the associated submission file using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surgical-pizza",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:36.873681Z",
     "start_time": "2022-02-15T15:34:36.188032Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_submission_file(SUBMISSION_PATH / \"sample_submission.csv\", df_obs_test.index, s_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-acrobat",
   "metadata": {},
   "source": [
    "# Constant baseline: 30 most observed species\n",
    "\n",
    "The first baseline consists in predicting the 30 most observed species on the train set which corresponds exactly to the \"Top-30 most present species\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "internal-bridal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:37.030364Z",
     "start_time": "2022-02-15T15:34:36.875285Z"
    }
   },
   "outputs": [],
   "source": [
    "species_distribution = df_obs.loc[obs_id_train][\"species_id\"].value_counts(normalize=True)\n",
    "top_30_most_observed = species_distribution.index.values[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-diagnosis",
   "metadata": {},
   "source": [
    "As expected, it does not perform very well on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "basic-gnome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:37.043069Z",
     "start_time": "2022-02-15T15:34:37.032104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-30 error rate: 93.5%\n"
     ]
    }
   ],
   "source": [
    "s_pred = np.tile(top_30_most_observed[None], (n_val, 1))\n",
    "score = top_k_error_rate_from_sets(y_val, s_pred)\n",
    "print(\"Top-30 error rate: {:.1%}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-shuttle",
   "metadata": {},
   "source": [
    "We will however generate the associated submission file on the test using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "handmade-bruce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:37.762255Z",
     "start_time": "2022-02-15T15:34:37.044524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute baseline on the test set\n",
    "n_test = len(df_obs_test)\n",
    "s_pred = np.tile(top_30_most_observed[None], (n_test, 1))\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission_file(SUBMISSION_PATH / \"constant_top_30_most_present_species_baseline.csv\", df_obs_test.index, s_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-hardwood",
   "metadata": {},
   "source": [
    "# Random forest on environmental vectors\n",
    "\n",
    "A classical approach in ecology is to train Random Forests on environmental vectors.\n",
    "\n",
    "We show here how to do so using [scikit-learn](https://scikit-learn.org/).\n",
    "\n",
    "We start by loading the environmental vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "criminal-tomorrow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:45.731902Z",
     "start_time": "2022-02-15T15:34:37.764022Z"
    }
   },
   "outputs": [],
   "source": [
    "df_env = pd.read_csv(DATA_PATH / \"pre-extracted\" / \"environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\n",
    "\n",
    "X_train = df_env.loc[obs_id_train].values\n",
    "X_val = df_env.loc[obs_id_val].values\n",
    "X_test = df_env.loc[obs_id_test].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-giving",
   "metadata": {},
   "source": [
    "Then, we need to handle properly the missing values.\n",
    "\n",
    "For instance, using `SimpleImputer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-nashville",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:34:46.705147Z",
     "start_time": "2022-02-15T15:34:45.734104Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imp.fit(X_train)\n",
    "\n",
    "X_train = imp.transform(X_train)\n",
    "X_val = imp.transform(X_val)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-sampling",
   "metadata": {},
   "source": [
    "We can now start training our Random Forest (as there are a lot of observations, over 1.8M, this can take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "wrong-landscape",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:37:23.045384Z",
     "start_time": "2022-02-15T15:34:46.706534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, n_estimators=16, n_jobs=-1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "est = RandomForestClassifier(n_estimators=16, max_depth=10, n_jobs=-1)\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-award",
   "metadata": {},
   "source": [
    "As there are a lot of classes (over 17K), we need to be cautious when predicting the scores of the model.\n",
    "\n",
    "This can easily take more than 5Go on the validation set.\n",
    "\n",
    "For this reason, we will be predict the top-30 sets by batches using the following generic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "textile-gabriel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:37:23.063650Z",
     "start_time": "2022-02-15T15:37:23.051514Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_predict(predict_func, X, batch_size=1024):\n",
    "    res = predict_func(X[:1])\n",
    "    n_samples, n_outputs, dtype = X.shape[0], res.shape[1], res.dtype\n",
    "    \n",
    "    preds = np.empty((n_samples, n_outputs), dtype=dtype)\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        preds[i:i+batch_size] = predict_func(X_batch)\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-nashville",
   "metadata": {},
   "source": [
    "We can know compute the top-30 error rate on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "insured-profession",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:38:04.919835Z",
     "start_time": "2022-02-15T15:37:23.068792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-30 error rate: 80.1%\n"
     ]
    }
   ],
   "source": [
    "def predict_func(X):\n",
    "    y_score = est.predict_proba(X)\n",
    "    s_pred = predict_top_30_set(y_score)\n",
    "    return s_pred\n",
    "\n",
    "s_val = batch_predict(predict_func, X_val, batch_size=1024)\n",
    "score_val = top_k_error_rate_from_sets(y_val, s_val)\n",
    "print(\"Top-30 error rate: {:.1%}\".format(score_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-mailing",
   "metadata": {},
   "source": [
    "We now predict the top-30 sets on the test data and save them in a submission file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "regulated-performer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-15T15:38:44.086269Z",
     "start_time": "2022-02-15T15:38:04.921407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute baseline on the test set\n",
    "s_pred = batch_predict(predict_func, X_test, batch_size=1024)\n",
    "\n",
    "# Generate the submission file\n",
    "generate_submission_file(SUBMISSION_PATH / \"random_forest_on_environmental_vectors.csv\", df_obs_test.index, s_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-diary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {
    "height": "856.867px",
    "left": "0px",
    "right": "1468px",
    "top": "111.133px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
